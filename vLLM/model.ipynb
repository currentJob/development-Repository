{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch 버전: 2.6.0+cu118\n",
      "CUDA 사용 가능 여부: True\n",
      "CUDA 버전: 11.8\n",
      "cuDNN 버전: 90100\n",
      "GPU 개수: 1\n",
      "GPU 이름: NVIDIA GeForce RTX 2070 with Max-Q Design\n",
      "현재 사용 중인 장치: 0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(\"Torch 버전:\", torch.__version__)\n",
    "print(\"CUDA 사용 가능 여부:\", torch.cuda.is_available())\n",
    "print(\"CUDA 버전:\", torch.version.cuda)\n",
    "print(\"cuDNN 버전:\", torch.backends.cudnn.version())\n",
    "print(\"GPU 개수:\", torch.cuda.device_count())\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU 이름:\", torch.cuda.get_device_name(0))\n",
    "    print(\"현재 사용 중인 장치:\", torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\f1soft\\anaconda3\\envs\\python-39\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\f1soft\\anaconda3\\envs\\python-39\\lib\\site-packages\\huggingface_hub\\file_download.py:832: UserWarning: `local_dir_use_symlinks` parameter is deprecated and will be ignored. The process to download files to a local folder has been updated and do not rely on symlinks anymore. You only need to pass a destination folder as`local_dir`.\n",
      "For more details, check out https://huggingface.co/docs/huggingface_hub/main/en/guides/download#download-files-to-local-folder.\n",
      "  warnings.warn(\n",
      "Fetching 9 files: 100%|██████████| 9/9 [01:14<00:00,  8.30s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\f1soft\\\\Desktop\\\\f1soft\\\\python\\\\development-Repository\\\\vLLM\\\\models\\\\DeepSeek-R1-Distill-Qwen-1.5B'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "snapshot_download(\n",
    "    repo_id=\"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\",\n",
    "    local_dir=\"./models/DeepSeek-R1-Distill-Qwen-1.5B\",\n",
    "    local_dir_use_symlinks=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "snapshot_download(\n",
    "    repo_id=\"deepseek-ai/deepseek-llm-7b-chat\",\n",
    "    local_dir=\"./models/deepseek-llm-7b-chat\",\n",
    "    local_dir_use_symlinks=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\f1soft\\anaconda3\\envs\\python-39\\lib\\site-packages\\huggingface_hub\\file_download.py:832: UserWarning: `local_dir_use_symlinks` parameter is deprecated and will be ignored. The process to download files to a local folder has been updated and do not rely on symlinks anymore. You only need to pass a destination folder as`local_dir`.\n",
      "For more details, check out https://huggingface.co/docs/huggingface_hub/main/en/guides/download#download-files-to-local-folder.\n",
      "  warnings.warn(\n",
      "Fetching 10 files: 100%|██████████| 10/10 [00:16<00:00,  1.64s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\f1soft\\\\Desktop\\\\f1soft\\\\python\\\\development-Repository\\\\vLLM\\\\models\\\\Llama-3.2-3B-KO-EN-Translation'"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "snapshot_download(\n",
    "    repo_id=\"kwoncho/Llama-3.2-3B-KO-EN-Translation\",\n",
    "    local_dir=\"./models/Llama-3.2-3B-KO-EN-Translation\",\n",
    "    local_dir_use_symlinks=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_content_after_think(text):\n",
    "    if \"</think>\" in text:\n",
    "        return text.split(\"</think>\")[-1].strip()\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def generate_quiz_from_subject(prompt):\n",
    "    url = \"http://localhost:8000/v1/chat/completions\"\n",
    "    payload = {\n",
    "        \"model\": \"/model\",\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": (\n",
    "                    \"You are an AI specialized in creating quiz questions. \"\n",
    "                    \"When the user provides a subject, generate one high-quality multiple-choice question related to that subject. \"\n",
    "                    \"Provide exactly five answer choices labeled A to E, and clearly indicate the correct answer at the end. \"\n",
    "                    \"Also, provide a brief explanation of why the correct answer is right. \"\n",
    "                    \"The question should be appropriate for general learners. \"\n",
    "                    \"Format your response as follows:\\n\\n\"\n",
    "                    \"Subject: [subject]\\n\"\n",
    "                    \"Question: [your question]\\n\"\n",
    "                    \"A. [option A]\\n\"\n",
    "                    \"B. [option B]\\n\"\n",
    "                    \"C. [option C]\\n\"\n",
    "                    \"D. [option D]\\n\"\n",
    "                    \"E. [option E]\\n\"\n",
    "                    \"Answer: [Correct option letter]\\n\"\n",
    "                    \"Explanation: [Why the answer is correct]\"\n",
    "                )\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt\n",
    "            }\n",
    "        ],\n",
    "        \"temperature\": 0.1,\n",
    "        \"max_tokens\": 1024\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.post(url, json=payload)\n",
    "        response.raise_for_status()\n",
    "        result = response.json()\n",
    "\n",
    "        content = result[\"choices\"][0][\"message\"][\"content\"]\n",
    "        clean_content = extract_content_after_think(content)\n",
    "        \n",
    "        return clean_content\n",
    "    \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(\"요청 실패:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:21<00:00, 10.59s/it]\n"
     ]
    }
   ],
   "source": [
    "# model_path = \"./models/Llama-3.2-3B-KO-EN-Translation\"\n",
    "model_path = \"kwoncho/Llama-3.2-3B-KO-EN-Translation\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"cuda:0\",\n",
    "    )\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(raw: str) -> str:\n",
    "    # 1. HTML 태그 제거\n",
    "    text = re.sub(r'<.*?>', '', raw)\n",
    "\n",
    "    # 2. '\\\\n' → 실제 줄바꿈\n",
    "    text = text.replace('\\\\n', '\\n\\n')\n",
    "\n",
    "    # 3. table 포함된 줄 제거\n",
    "    text = '\\n'.join(line for line in text.splitlines() if 'table' not in line.lower())\n",
    "\n",
    "    # 4. '*' 문자 제거\n",
    "    text = text.replace('*', '')\n",
    "\n",
    "    # 5. 여러 공백 → 하나의 공백\n",
    "    text = re.sub(r'[ \\t]+', ' ', text)\n",
    "\n",
    "    # 6. 보기 항목 줄바꿈 (A. ~ E.)\n",
    "    text = re.sub(r'\\s?A\\.', r'\\nA.', text)\n",
    "    text = re.sub(r'\\s?B\\.', r'\\nB.', text)\n",
    "    text = re.sub(r'\\s?C\\.', r'\\nC.', text)\n",
    "    text = re.sub(r'\\s?D\\.', r'\\nD.', text)\n",
    "    text = re.sub(r'\\s?E\\.', r'\\nE.', text)\n",
    "\n",
    "    # 7. 연속 줄바꿈 정리\n",
    "    text = re.sub(r'\\n{2,}', '\\n\\n', text)\n",
    "\n",
    "    # 8. 양쪽 공백 제거\n",
    "    text = text.strip()\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"**Question:** What is a key feature of Python's object-oriented programming model?\\n\\n**A.** Supports recursion  \\n**B.** Includes built-in libraries  \\n**C.** Emphasizes strong support for object-oriented programming  \\n**D.** Is used for web development  \\n**E.** A simple programming language  \\n\\n**Answer:** C. Emphasizes strong support for object-oriented programming  \\n\\n**Explanation:** Python's object-oriented programming model is a fundamental aspect of the language, making it widely used in software development. This feature allows for better structure and organization of code, which is a key distinction from other programming languages.\""
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = generate_quiz_from_subject(\"Python\")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \\\n",
    "    f\"\"\"\n",
    "    ### Instruction:\n",
    "    주어진 텍스트를 한국어로 번역하세요.\n",
    "    ### Input:\n",
    "    '''{response}'''\n",
    "    ### Output:\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(model.device)\n",
    "output = model.generate(\n",
    "    input_ids,\n",
    "    max_new_tokens=200,\n",
    "    temperature=0.1,\n",
    "    top_p=.9,\n",
    "    repetition_penalty=1.3,\n",
    "    do_sample=True,\n",
    "    eos_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: 파이썬의 객체지향 프로그래밍 모델에서 중요한 특징은 무엇인가?\n",
      "\n",
      "A. 리CURSION을 지원하는 것 (Recursive functions)\n",
      "\n",
      "B. 내장 라이브러리(Internal library)가 포함되어 있다.\n",
      "\n",
      "C. 강력한 객체 지향 프로그래밍에 대한 지원을 중시한다.\n",
      "\n",
      "D. 웹 개발용으로 사용된다.\n",
      "\n",
      "E. 단순한 프로그램 언어(Simple programing languauge)\n",
      " ANSWER:\n",
      "C. 강력한 객체 지향 프로그래밍을 중시함\n",
      " EXPLANATION: 파이썬의 객체지향프로그래밍모델은 소프트웨어개발 에서 널리 쓰이는 주요특성이며 이 기능은 다른 프 로그램링구어들과 비교하여 코드 구조 및 조직화 측면에서 큰 차이를 가져\n"
     ]
    }
   ],
   "source": [
    "raw_string = generated_text.split(\"Output:\")[1]\n",
    "response = clean_text(raw_string)\n",
    "\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python-39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
