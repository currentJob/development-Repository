{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch 버전: 2.6.0+cu118\n",
      "CUDA 사용 가능 여부: True\n",
      "CUDA 버전: 11.8\n",
      "cuDNN 버전: 90100\n",
      "GPU 개수: 1\n",
      "GPU 이름: NVIDIA GeForce RTX 2070 with Max-Q Design\n",
      "현재 사용 중인 장치: 0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(\"Torch 버전:\", torch.__version__)\n",
    "print(\"CUDA 사용 가능 여부:\", torch.cuda.is_available())\n",
    "print(\"CUDA 버전:\", torch.version.cuda)\n",
    "print(\"cuDNN 버전:\", torch.backends.cudnn.version())\n",
    "print(\"GPU 개수:\", torch.cuda.device_count())\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU 이름:\", torch.cuda.get_device_name(0))\n",
    "    print(\"현재 사용 중인 장치:\", torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "snapshot_download(\n",
    "    repo_id=\"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\",\n",
    "    local_dir=\"./models/DeepSeek-R1-Distill-Qwen-1.5B\",\n",
    "    local_dir_use_symlinks=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "snapshot_download(\n",
    "    repo_id=\"deepseek-ai/deepseek-llm-7b-chat\",\n",
    "    local_dir=\"./models/deepseek-llm-7b-chat\",\n",
    "    local_dir_use_symlinks=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "snapshot_download(\n",
    "    repo_id=\"kwoncho/Llama-3.2-3B-KO-EN-Translation\",\n",
    "    local_dir=\"./models/Llama-3.2-3B-KO-EN-Translation\",\n",
    "    local_dir_use_symlinks=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_content_after_think(text):\n",
    "    if \"</think>\" in text:\n",
    "        return text.split(\"</think>\")[-1].strip()\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def generate_quiz_from_subject(prompt):\n",
    "    url = \"http://localhost:8000/v1/chat/completions\"\n",
    "    payload = {\n",
    "        \"model\": \"/model\",\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": (\n",
    "                    \"You are an AI specialized in creating quiz questions. \"\n",
    "                    \"When the user provides a subject, generate one high-quality multiple-choice question related to that subject. \"\n",
    "                    \"Provide exactly five answer choices labeled A to E, and clearly indicate the correct answer at the end. \"\n",
    "                    \"Also, provide a brief explanation of why the correct answer is right. \"\n",
    "                    \"The question should be appropriate for general learners. \"\n",
    "                    \"Format your response as follows:\\n\\n\"\n",
    "                    \"Subject: [subject]\\n\"\n",
    "                    \"Question: [your question]\\n\"\n",
    "                    \"A. [option A]\\n\"\n",
    "                    \"B. [option B]\\n\"\n",
    "                    \"C. [option C]\\n\"\n",
    "                    \"D. [option D]\\n\"\n",
    "                    \"E. [option E]\\n\"\n",
    "                    \"Answer: [Correct option letter]\\n\"\n",
    "                    \"Explanation: [Why the answer is correct]\"\n",
    "                )\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt\n",
    "            }\n",
    "        ],\n",
    "        \"temperature\": 0.1,\n",
    "        \"max_tokens\": 1024\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.post(url, json=payload)\n",
    "        response.raise_for_status()\n",
    "        result = response.json()\n",
    "\n",
    "        content = result[\"choices\"][0][\"message\"][\"content\"]\n",
    "        clean_content = extract_content_after_think(content)\n",
    "        \n",
    "        return clean_content\n",
    "    \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(\"요청 실패:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:41<00:00, 20.72s/it]\n"
     ]
    }
   ],
   "source": [
    "model_path = \"./models/Llama-3.2-3B-KO-EN-Translation\"\n",
    "# model_path = \"kwoncho/Llama-3.2-3B-KO-EN-Translation\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"cuda:0\",\n",
    "    )\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(raw: str) -> str:\n",
    "    text = re.sub(r'<.*?>', '', raw)\n",
    "    text = text.replace('\\\\n', '\\n\\n')\n",
    "    text = '\\n'.join(line for line in text.splitlines() if 'table' not in line.lower())\n",
    "    text = text.replace('*', '')\n",
    "    text = re.sub(r'[ \\t]+', ' ', text)\n",
    "    text = re.sub(r'\\s?A\\.', r'\\nA.', text)\n",
    "    text = re.sub(r'\\s?B\\.', r'\\nB.', text)\n",
    "    text = re.sub(r'\\s?C\\.', r'\\nC.', text)\n",
    "    text = re.sub(r'\\s?D\\.', r'\\nD.', text)\n",
    "    text = re.sub(r'\\s?E\\.', r'\\nE.', text)\n",
    "    text = re.sub(r'\\n{2,}', '\\n\\n', text)\n",
    "    text = text.strip()\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'**Question:**  \\nWhat is Python primarily used for?  \\n\\nA. Programming  \\nB. Software development  \\nC. Both programming and software development  \\nD. None of the above  \\nE. All of the above  \\n\\n**Answer:**  \\nB. Software development  \\n\\n**Explanation:**  \\nPython is primarily used for both programming and software development. While it is a programming language, its versatility extends to software development, making option B the most accurate choice.'"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = generate_quiz_from_subject(\"Python\")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \\\n",
    "    f\"\"\"\n",
    "    ### Instruction:\n",
    "    주어진 텍스트를 한국어로 번역하세요.\n",
    "    ### Input:\n",
    "    '''{response}'''\n",
    "    ### Output:\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(model.device)\n",
    "output = model.generate(\n",
    "    input_ids,\n",
    "    max_new_tokens=200,\n",
    "    temperature=0.1,\n",
    "    top_p=.95,\n",
    "    repetition_penalty=1.3,\n",
    "    do_sample=True,\n",
    "    eos_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: 파이썬은 무엇에 사용되는가?\n",
      "\n",
      "A. 프로그래밍\n",
      "\n",
      "B. 소프트웨어 개발\n",
      "\n",
      "C. 프로그램과 소프트웨어 개발의 양쪽 모두\n",
      "\n",
      "D. 위와 같은 것 중ใด도 아니고\n",
      "\n",
      "E. 모든 것을 위해 사용된다\n",
      " 답자:\n",
      "B. 소프트웨어 개발\n",
      " 설명 : 파이는 프로그래밍 언어로서 그 외에도 다양한 용도로 활용되며, 이외에는 아니다는 점에서 옵션 B가 가장 적절하다.\n",
      " \n",
      "\n",
      " Below are two more examples that demonstrate various ways you might utilize Pandas libraries within your own projects or scripts:\n"
     ]
    }
   ],
   "source": [
    "raw_string = generated_text.split(\"Output:\")[1]\n",
    "response = clean_text(raw_string)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python-39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
