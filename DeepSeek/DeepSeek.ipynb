{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Feb 19 14:59:49 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 572.16                 Driver Version: 572.16         CUDA Version: 12.8     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                  Driver-Model | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 2070 ...  WDDM  |   00000000:01:00.0  On |                  N/A |\n",
      "| N/A   77C    P8             11W /   80W |     162MiB /   8192MiB |     25%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A             256    C+G   ...iceHub.ThreadedWaitDialog.exe      N/A      |\n",
      "|    0   N/A  N/A            8132    C+G   ...munity\\Common7\\IDE\\devenv.exe      N/A      |\n",
      "|    0   N/A  N/A           16872      C   ...da3\\envs\\python-39\\python.exe      N/A      |\n",
      "+-----------------------------------------------------------------------------------------+\n",
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2022 NVIDIA Corporation\n",
      "Built on Wed_Sep_21_10:41:10_Pacific_Daylight_Time_2022\n",
      "Cuda compilation tools, release 11.8, V11.8.89\n",
      "Build cuda_11.8.r11.8/compiler.31833905_0\n",
      "C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.8\\bin\\nvcc.exe\n",
      "C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v12.3\\bin\\nvcc.exe\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi\n",
    "!nvcc --version\n",
    "!where nvcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0+cu118\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\f1soft\\anaconda3\\envs\\python-39\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.49.0\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:22<00:00, 11.44s/it]\n",
      "Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# DeepSeek 모델 로드\n",
    "model_name = \"deepseek-ai/deepseek-llm-7b-chat\"\n",
    "\n",
    "# 토크나이저(Tokenization) 로드\n",
    "# - 모델과 동일한 이름의 토크나이저를 불러옴\n",
    "# - 입력 문장을 토큰으로 변환하고 모델의 출력을 다시 문장으로 변환하는 역할\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# 사전 학습된 LLM (Causal Language Model) 로드\n",
    "# - torch_dtype=torch.float16: 16-bit 부동소수점(FP16)으로 모델을 로드하여 메모리 사용량을 최적화\n",
    "# - device_map=\"auto\": 사용 가능한 GPU 또는 CPU를 자동으로 감지하여 모델을 배치\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, \n",
    "                                             torch_dtype=torch.float16,  # 메모리 절약을 위한 FP16 설정\n",
    "                                             device_map=\"auto\")  # 적절한 장치(CPU/GPU)로 자동 할당"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(question: str) -> str:\n",
    "    \"\"\"\n",
    "    질문을 입력받아 DeepSeek 으로 응답을 생성하는 함수\n",
    "\n",
    "    :param question: 사용자 입력 질문 (문자열)\n",
    "    :return: 모델이 생성한 응답 (문자열)\n",
    "    \"\"\"\n",
    "    \n",
    "    # 입력 텍스트를 토큰화 (문장을 모델이 이해할 수 있는 토큰으로 변환)\n",
    "    # - return_tensors=\"pt\": PyTorch 텐서 형식으로 변환\n",
    "    # - padding=True: 문장의 길이를 맞추기 위해 패딩 적용\n",
    "    # - truncation=True: 문장이 너무 길면 자르기\n",
    "    inputs = tokenizer(question, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "    # 토큰화된 입력 데이터를 GPU로 이동 (CUDA 사용)\n",
    "    input_ids = inputs.input_ids.to(\"cuda\")\n",
    "    attention_mask = inputs.attention_mask.to(\"cuda\")\n",
    "\n",
    "    # 모델을 실행하여 응답 생성\n",
    "    # - model.generate(): 주어진 입력에 대해 모델이 새로운 텍스트를 생성\n",
    "    # - max_length=200: 최대 200 토큰까지 출력 (너무 길어지는 걸 방지)\n",
    "    # - pad_token_id: 패딩 토큰을 EOS 토큰으로 설정\n",
    "    output_ids = model.generate(input_ids,\n",
    "                                attention_mask=attention_mask,\n",
    "                                max_length=200,\n",
    "                                pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "    # 생성된 토큰을 사람이 읽을 수 있는 텍스트로 디코딩\n",
    "    # - skip_special_tokens=True: <EOS>, <PAD> 같은 특수 토큰 제거\n",
    "    output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    # 결과 출력 (디버깅용)\n",
    "    print(\"DeepSeek AI Output:\")\n",
    "\n",
    "    return output_text  # 최종 응답 반환\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def measure_time(func, *args, **kwargs):\n",
    "    \"\"\"\n",
    "    실행할 함수와 인자를 받아 실행 시간을 측정하는 함수\n",
    "    \n",
    "    :param func: 실행할 함수\n",
    "    :param args: 함수의 위치 인자\n",
    "    :param kwargs: 함수의 키워드 인자\n",
    "    :return: (함수 실행 결과, 실행 시간)\n",
    "    \"\"\"\n",
    "    start_time = time.perf_counter()  # 시작 시간 측정\n",
    "    result = func(*args, **kwargs)  # 함수 실행\n",
    "    end_time = time.perf_counter()  # 종료 시간 측정\n",
    "    \n",
    "    elapsed_time = end_time - start_time  # 실행 시간 계산\n",
    "    return result, elapsed_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeepSeek AI Output:\n",
      "한글로 대화 가능해?\n",
      "\n",
      "아니요, 현재 이 노말 데모는 한글로 대화할 수 없습니다. 다음 릴리스 시에 추가할 예정입니다.\n",
      "Execution Time: 163.1209 seconds\n"
     ]
    }
   ],
   "source": [
    "question = \"한글로 대화 가능해?\"\n",
    "response, execution_time = measure_time(generate_response, question)\n",
    "\n",
    "print(response)\n",
    "print(f\"Execution Time: {execution_time:.4f} seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python-39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
